{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "import umap.umap_ as umap_\n",
    "import hdbscan\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.patches as mpatches\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Supondo que 'X_tsne' seja a matriz de embeddings do t-SNE\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import os\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names_out()\n",
    "    labels_ = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels_)}\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                   .Doc\n",
    "                   .count()\n",
    "                   .reset_index()\n",
    "                   .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                   .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "\n",
    "def map_labels_to_colors(labels,cmap_):\n",
    "    cmap = plt.get_cmap(cmap_)\n",
    "    num_labels = np.max(labels) + 1\n",
    "    colors = cmap(np.linspace(0, 1, num_labels))\n",
    "    # print(colors)\n",
    "    return colors[labels]\n",
    "\n",
    "\n",
    "def gera_comparacao(list_preprossing_,cluster_):\n",
    "    docs_df = pd.DataFrame(list_preprossing_,columns=[\"Doc\"])\n",
    "    docs_df['Topic'] = cluster_\n",
    "    docs_df['Doc_ID'] = range(len(docs_df))\n",
    "    docs_per_topic=docs_df.groupby([\"Topic\"],as_index=False).agg({\"Doc\":' '.join})\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(list_preprossing_))\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=5)\n",
    "    # print(top_n_words)\n",
    "\n",
    "    topic_sizes = extract_topic_sizes(docs_df)\n",
    "    legends=[top_n_words[i][0] for i in top_n_words]\n",
    "    return  legends\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Conversão para minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remoção de pontuações\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "    # Remoção de caracteres especiais, preservando letras acentuadas\n",
    "    text = re.sub(r'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôûÂÊÎÔÛàèìòùÀÈÌÒÙãõñÃÕÑçÇ\\s]+', '', text)\n",
    "    text = unidecode(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"/home/rafael/Documentos/FACOM/Douturado/Doutorado/webcrawler/medicamentos.xlsx\",index_col=0,dtype=str)\n",
    "df_train.head()\n",
    "list_preprossing=[]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "for dta in range(df_train.shape[0]):\n",
    "    # value=str(df_train.iloc[dta,0])\n",
    "    # value=str(df_train.iloc[dta,0])+\" \"+str(df_train.iloc[dta,1])\n",
    "    value=str(df_train.iloc[dta,0])+\" \"+str(df_train.iloc[dta,1]) +\" \"+str(df_train.iloc[dta,2])+\" \"+str(df_train.iloc[dta,3])\n",
    "    list_preprossing.append(preprocess_text(value))\n",
    "list_preprossing=np.array(list_preprossing)\n",
    "list_preprossing=list_preprossing[list_preprossing!='nan']\n",
    "print(list_preprossing)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# embedder = SentenceTransformer('distilbert-base-nli-mean-tokens',device='cuda')\n",
    "embedder = SentenceTransformer('all-mpnet-base-v2')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus_embeddings = embedder.encode(list_preprossing,show_progress_bar=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state = 42, n_components=2,perplexity=5,metric='cosine')\n",
    "pca = PCA(n_components=2)\n",
    "umap= umap_.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine')\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "embeddings2d_pca = pca.fit_transform(corpus_embeddings)\n",
    "embeddings2d_tsne = tsne.fit_transform(corpus_embeddings)\n",
    "embeddings2d_umap =  umap.fit_transform(corpus_embeddings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "color_maps = 'tab20'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "embeddings2d_dic ={'pca':embeddings2d_pca,'tsne':embeddings2d_tsne,'umap':embeddings2d_tsne}"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "for key in embeddings2d_dic:\n",
    "    print(key)\n",
    "    for k in range(4, 20, 4):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=40)\n",
    "        kmeans_ = kmeans.fit(embeddings2d_dic[key])\n",
    "        result = pd.DataFrame(embeddings2d_dic[key], columns=['x', 'y'])\n",
    "        result['labels'] = kmeans_.labels_\n",
    "        centroids = kmeans_.cluster_centers_\n",
    "\n",
    "        labels = gera_comparacao(list_preprossing, kmeans_.labels_)\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        plt.scatter(result.x, result.y, c=result.labels, cmap=color_maps)\n",
    "\n",
    "        # Calcular a distância média entre o centróide e os pontos do grupo\n",
    "        distances = pairwise_distances(result[['x', 'y']], centroids)\n",
    "        avg_distance = np.mean(distances, axis=1)\n",
    "\n",
    "        # Ajustar o raio multiplicando por um fator\n",
    "        radius_factor = 0.2\n",
    "        adjusted_radius = avg_distance * radius_factor\n",
    "\n",
    "        # Adicionar círculos representando os centróides com raio ajustado\n",
    "        for centroid, radius in zip(centroids, adjusted_radius):\n",
    "            circle = plt.Circle((centroid[0], centroid[1]), radius=radius, color='red', fill=False)\n",
    "            ax.add_artist(circle)\n",
    "\n",
    "            # Adicionar marcador para o centro do centróide\n",
    "            plt.scatter(centroid[0], centroid[1], c='black', s=100, marker='x')\n",
    "\n",
    "        plt.colorbar()\n",
    "        legend_labels = [label for label in labels]\n",
    "        legend_colors = map_labels_to_colors(np.arange(len(legend_labels)), color_maps)\n",
    "        legend_elements = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "        plt.legend(handles=legend_elements)\n",
    "        plt.legend(legend_elements, legend_labels)\n",
    "        fig.savefig(f\"./{key}/{key}-kmeans-{k}.png\")\n",
    "        plt.close(fig)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for key in embeddings2d_dic:\n",
    "    print(key)\n",
    "    for k in range (5,9):\n",
    "        cluster = hdbscan.HDBSCAN(min_cluster_size=k,\n",
    "                              metric='euclidean',\n",
    "                              cluster_selection_method='eom').fit(embeddings2d_dic[key])\n",
    "        result = pd.DataFrame(embeddings2d_dic[key], columns=['x', 'y'])\n",
    "        result['labels'] = cluster.labels_\n",
    "        outliers = result.loc[result.labels == -1, :]\n",
    "        clustered = result.loc[result.labels != -1, :]\n",
    "        labels = gera_comparacao(list_preprossing,cluster.labels_)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "        # plt.scatter(exemplars[:, 0], exemplars[:, 1], c='r' ,marker='^')\n",
    "        # plt.scatter(outliers.x, outliers.y, c='black' ,marker='x')\n",
    "        plt.scatter(clustered.x, clustered.y,c=clustered.labels,  cmap=color_maps)\n",
    "        plt.colorbar()\n",
    "        legend_labels = [label for label in labels]\n",
    "        legend_colors = map_labels_to_colors(np.arange(len(legend_labels)),color_maps)\n",
    "        legend_elements = [mpatches.Patch(color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "        plt.legend(handles=legend_elements)\n",
    "        plt.legend(legend_elements, legend_labels)\n",
    "        fig.savefig(f\"./{key}/{key}-hdbscan-{k}.png\")\n",
    "        plt.close(fig)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "k_values=range(5, 100)\n",
    "print(k_values)\n",
    "for key in embeddings2d_dic:\n",
    "    silhouette_scores = []\n",
    "    calinski_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    for k in  k_values :\n",
    "        kmeans = KMeans(n_clusters=k,n_init=40)\n",
    "        kmeans.fit(embeddings2d_tsne)\n",
    "        labels = kmeans.labels_\n",
    "        silhouette_avg = silhouette_score(embeddings2d_dic[key], labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        calinski_score = calinski_harabasz_score(embeddings2d_dic[key], labels)\n",
    "        davies_bouldin_score = davies_bouldin_score(embeddings2d_dic[key], labels)\n",
    "        calinski_scores.append(calinski_score)\n",
    "        davies_bouldin_scores.append(davies_bouldin_score)\n",
    "\n",
    "    fig,ax =plt.subplots(1,figsize=(20,10))\n",
    "    ax.plot(k_values, silhouette_scores, marker='o')\n",
    "    plt.xlabel('Número de clusters (k)')\n",
    "    plt.ylabel('Coeficiente de Silhueta Médio')\n",
    "    fig.savefig(f\"./{key}/{key}-kmeans-silhouette.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    plt.plot(k_values, calinski_scores, marker='o')\n",
    "    plt.xlabel('Número de clusters (k)')\n",
    "    plt.ylabel('Calinski-Harabasz Score')\n",
    "    fig.savefig(f\"./{key}/{key}-kmeans-calinski.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Plotar gráfico da métrica de Davies-Bouldin\n",
    "    plt.plot(k_values, davies_bouldin_scores, marker='o')\n",
    "    plt.xlabel('Número de clusters (k)')\n",
    "    plt.ylabel('Davies-Bouldin Score')\n",
    "    fig.savefig(f\"./{key}/{key}-kmeans-davies.png\")\n",
    "    plt.close(fig)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
